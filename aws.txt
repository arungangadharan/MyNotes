Region is a geographical location
Avalability zone is a Data center. A region can have multiple AZs (2 or more)
Edge location is used for caching services. it consists of Clound Front - amaxon's content delivery network

COMPUTE
--------
EC2 is elastic cloud service - only VMs
EC2 Container service - is docker services
Elastic bean stack is for someone who does not know AWS
Lambda - does things for you. You need to worry only about code. Text over image
Lighsail will give you a fixed IP adress + RDP/SSH access
Batch : Batch computing

STORAGE
-------
S3 - simple storage service. upload data into bucket
EFS : elastic file system
Glacier : data archiving
snowball - bring large data to data center; it is a physical device
storage gateway is put in DC or headoffice, which will be replicating data in S3

DATABASE
-------
RDS : relation database service, any RDBMS
Dynamo DB : non relational DB
Elasticache : cache queries
Red Shift : Intelligence - complex queries

Migration
---------
AWS migration hub : tracks migration of applications to AWS
application discovery service : detects applications and dependencies
DB migration services - for DB migration
server migration service : helps in migrating services to AWS

NETWORKING & Content delivery
-----------------------------
VPC : virtual private cloud - network configurations
Cloud Front - content delivery network
Route 55 - DNS service
API gateway : creating your APIs for other service
Direct Connect : dedicated line from different networks

DEVELOPER TOOLS
---------------
Code Star : project code management- CI/CD
Code Commit : code storage. source control
Code Build : compile and test
Code Deploy - code deployment
Code pipeline - CD system
X-ray : for debugging
Cloud 9 : IDE for development

MANAGEMENT
----------
cloud watch: monitoring services
cloud formation : way of scripting infrastructure
cloud trail : log changes to AWS environment
config : monitors the configuration of AWS infrastructure
OpsWork: automation of configuration of environment
service catalog : way of managing catalogs of IT services
Systems manager : patch maintainance sort of things
trusted advisor : gives advices by analyzing the AWS environment
Managed services : manages services

Media services
--------------
elastic transcoder : video processing like widening
media covert :firewall based to transcoding. 
Media live : live video streams
Media Package : video over internet
Media Store : storage for video
Media tailor : adding ads to video streams

Machine Learning
----------------
Sage maker : deep learning
comprehend : sentiment analysis
deeplens : AI camera
Lex: alexa's engine
machine learning : entry level learning
Polly : text to speech
rekognition : says what is in an uploaded file
amazon translate : transalator
transcribe : automatic speech recognition

Analytics
-----------
athena : runs SQL queries
EMR : elastic map reduce : for big data solutions
cloud search
elastic search service: search services for AWS
Kinesis : injecting large amouts of data to AWS
Kinesis video streams : injects videos and processing
quick sight : BI tool
Data pipeline : moving data between different AWS services
Glue : ETL tool service

Security, Identity and Compliance
----------------------------------
IAM : Identity access management - 
cognito - device authentication
GuardDuty : monitors malicious activities
Inspector : agent - for running tests. scheule security tests
Macie : will scan your S3 buckets - PI details
certificate Manager : you get SSL certificte
Clous HSM : Hardware security module. store PKs (private and public)
Directoy service : integrating microsoft AD services
Web application firewall : application layer security
Shield : prevents D-DOS attacks
Artifact : portal for AWS compliance report

MOBILE SERVICES
--------------
Mobile Hub : management console for mobility apps services
Pinpoint : way to use targetted push notifications
AWS AppSync : Updates data in web and mobile application platforms
Device farm : testing application in multiple devices
Mobile analytics : Analytics for mobile apps

AR & VR
-------
Sumerian : language for AR/VD/3D application design
"ready player one"

Application Integration
-----------------------
step function : way of managing lander functions
MQ : message queues
SNS : Notification service
SQS : decoupliging infra
SWF : simple workflow service - create simple workflows

Customer engagement
-------------------
Connect : Call center in cloud
Simple email service: email service - sending large amounts of emails

Business Productivity
--------------------
Alexa for business : lot of automation of services, like booking a CR..etc
Chime : Video conferecncing
work docs - drop box for work related data
work mail : using email through amazon

Desktop & App streaming
----------------------
workspaces : VDI solutions
app steam 2.0 : streaming actual applications like citrix

Internet of Things
-----------------
iOT
iot device management
amazon free RTOS - OS for micro controllers
green grass : s/w local compute messaging, interfacing capabilities

Game development
-----------------
Game lift : service to develop games

DETAILS
=========
IAM
- Centralized control of your AWS account
- shared access to your AWS account
- Granular permissions
- Identity Federation
- multi factor authentication
- temp access to devices
- helps in password rotation policy
- integrates with different AWS services
- supports PCI DSS compliance
- IAM does not have a region
- users
- groups
- roles
- policies

S3
- simple storage service
- Object based storage
- saves files
- Block based  - for programs
- 0 to 5 TB
- stored in bckets
- each bucket has a DNS name - globally unique
- region.amazonaws.com/bucket name
- HTTP 200 - success
- Data consistency model
	- read after write consistency for PUTS of new objects means. a new object creation will be reflected immediately
	- eventual consistency for overwrite PUTS and DELETES 9 - will take time to sync
-  storage is key-value pair. Key is the name of the file and data is a sequence of bytes of data (content of the file)
- then we have version as well
- metadata as well
- sub-resources
	- access control list (ROLE BASED PERMISSION)
	- Torrent (for torrenting)
- 99.99% availability
- 99.9999999% durability
- Tiered storage available
- life cycle management
- versioning
- encryption
- Secure data using ACL and bucket policies
- S3 standard : 99.99 availability and 99.9999999% durability. acessable across multiple devices in multiple facilities. designed to sustain the loss of 2 facilities
- S3 - IA :Infrequently accessed. for data that does not get accessed often. Lower fees than standard, but retrieval is charged
- S3 one Zone - IA : infrequent accessed which does not require multiple availability zone resilience
- Glacier : Very cheap - only for archiving
	- expedited :  retrieval takes only few minutes
	- Standard : retrieval takes 3-5 hours
	- Bulk : Retrieval takes 5-12 hours
Charging for
	- storage
	- requests made to the resource
	- Storage management pricing : Who owns and who can access
	- Data transfer pricing : for transferring across different regions
	- tranfer acceleration : for using high speed transfer network in amazon
- versioning cannot be disabled (once enabled), only suspended
- frequent changing does not need versioning
- upload the same file again, it gets saved as a different version
- you can delete any version
- versioning stop deleting functioning op as well
- if the file is deleted, it is still available in version with a tag ' with a delete marker'. if we go and delete the file of that kind, it just deletes the delete marker and restores the original file. So the front end delete does not delete the file. since it maintains multiple versions of the same file, it is not ideal to enable versioning of frequently changing files
- MFA delete is an option to enable multi factor authentication for versioning and delete operations. this can make the operations secure
- this stops people from accidently deleting files

Cross region replication
- to enable this, we need to have versioning enabled first
- this cross region replication, can be done across AWS account - can cover AWS acocunt of others as well
- while mentioning replication policy, we can mention the storage class as well. We usually mark it as 'Stanard - IA' if you want to use it as a backup with infrequent access
- it also requires an IAM role.
- when you enable replocation, only new and modified objects are going to be moved to the new bucket
- how do copy objects of one bucket to another bucket. this is easy to do over command line


AWS CLI
- add a usr through UI
- get program access, not console access
- enter access key id and secrett access key
- default region, 
- aws s3 ls - will list the buckets
- copying to another bucket : aws s3 cp --recursive s3://<from-bucket> s3://<to-bucket>
- when you delete a file in source, destination file also get deleted, however if we cancel delete in source bucket the destimation is not replicated
- Update operations also get updated in destination bucket
- deleting a latest version in source does not delete a verrsion in destination
- replication cannot be done across multiple buckets

Configuring transtions
- this canconfigure to move current version to stanard IA after a certain period and then to glacier
- craete a bucket. go to management > life cycle > add life cycle policies
- we can add tags along with life cycle policy
- add transition to the current version. can just say transition the current version to IA after 30 days and to glacier after 60 days
- can do the same with previous versions as well
- also we can say object expiry. Can say that object to be marked as expired after a period of time



Cloud front/CDN
- CDN is a system of distributed servers (network) that deliver web pages and other web contet to a user based on the geographic locations of the user, the origin of the web page and content delivery server
- Edge location is the place where data is cached, Orgin is where data is originally saved. Distribution is the network of edge locations
- when a user accesses data in a remote location, at first the data fecthing is slow. howver after first fectch, the data get stored in an edge location. After that, repeated request will fetch data from the edge location only.
- edge locations support write operations as well. You can create an object in an edge location and it gets automatically replicated in origin
- set a TTL for an object : Time to Live. once that's done, the object will be available in cache. it cannot be removed till TTL completes. However, you can clear the cache, however you will be charged for that.

- to create a distribution
	- go to cloud front
	- create distribution; there are two types - web and RTMP. web uses HTTP/HTTPS communication. RTMP is for adobe tools
	- Origin domain name is the name of the origin bucket
	- Origin path is a folder within bucket
	- Origin ID is just a name
	- restrict bucket access - says that public cannot access the content using the origin bucket URI. everyone has to use edge location for that
	- Origin access identity - need to check
	- Grant Permissions on bucket -if yes, it automatically updates policy file
	- Path pattern can be used to mention the type of file accessible
	- viwer protocol policy can be used to mention HTTP, re-direct HTTP folks to HTTPS or HTTPS only
	- Allowed HTTP methods. mention allowed HTTP methods (GET, POST..etc)
	- Can mention caching mechanism - custom ot orgin cache headers
	- can mention minimum and maximum TTL (time to live). this should be in seconds
	- restrict viewer access(use signed URLs or signed cookies )- need to check
	- AWS WAF Web ACL is for restricting access  - need to check
	- CNAMEs are alternate human readble name - generated names are tough to read
	- custom error pages possible -> under error pages tab
	- geography based restrcition also is possible - under Restrictions tab
	- geo- restrictions - you can either white list or black list
	- invalidations - for invalidatig objects from an edge locations


Security and Encryption
- bucket related restrcitions can be applied using bucket policies and ACLs
- buckets can have access logs which logs all requests made to a bucket. This logging can be configured to another bucket
- encryption can be done
	- in transit (while sending data into the buckets)
		- using SSL/TLS
	- At Rest
		- server side encryption
			- S3 managed keys - SSE S3
			- using AWS Key management service, Managed keys - SSE - KMS. envelo key, audit trail of key access
			- with customer provided keys - SSE-C
		- Client side encryption

Storage Gateway (This needs re-look)
-  this is used for syncing your on-premise server data with AWS services. this Data will be moved to IA and then to Glacier
- you get storage gateway as a VM image - it supports MS Hyper-V or VMware ESXI
- there are four types of storage gateways
	- File gateways (NFS)
		- NFS will be mounted on App server machine
		- App server connects to the Storage gateway VM - which connects to Amazon S3 using internet or Direct connect or Amazon VPC
	- Volume gateways (iSCSI) : not for flatfiles, block based storage
		- Data will be written to a virtual  HDD which will be backed up as point-in-time snapshots of your volumes and stored in clliud as 		Amazon EBS snapshots. this is just incremental back up - only covers changes - and also compressed
		- Stored Volumes : entire data on premise
			- entire data is written on your on-premise data storage which gets backed up as Amazon ESBs.size possible is 1 GB - 16 GBs
		- Cached volumes : only recently accessed data is kepy on premise. Majority of the data is in S3. so the on-premise h/w requirement is 		less
	- Tape gateways (VTL) : virtual tapes - use lifecycle to glacier 
		- back of the data is done using NetBackup, Backup Exec or Veeam which will be connected to gateway VM using iSCSI, which in turn will be 		backed up in S3 and follows life cycle
	 			 
Snob Ball
- when you want to send huge amount of data, you use and external transferrable device. This is called snow ball. This is more like an external HDD. deals with peta bytes of data.
- Three types
	- Standard snow ball
		- 80 TB is available. tamper resistant enclosures, 256 bit encryption, ad comes with industry trusted platform module (TPM), to ensure 		both security and full chain of custody of your data (you will be able to know where snow-ball at this moment is
	- Snowball edge
		- 100 TB. like snowball standard, but has computing capabilities as well. Use case : airlines. Edge can be used to monitor airline engine 		and when it lands, it can be send to AWS center
	- snowmobile
		- more like a truck. Capacity 100 peta bytes. 
- order a snowball. connect power and network
- install CLI for snowball
- get credentials - oyu get client unlock key and manifest file
- ./snowball start -i 192.168.1.116 -m <manifest file> -u <unlock key>
- snowball cp hello.txt s3://<bucket name from snow ball details - browser>
- ./snowball stop
- do arrangement for sending snowball back to AWS

S3 ransfer acceleration
- this is used for improving your upload speed. Instead of uploading directly to S3 bucket, we will upload it to an edge location which will be transferred to S3 location
- You will get a edge location URL for it -> <bucket name>.s3-accelerate.amazonaws.com
- go to your bucket, properties > Transfer acceleration - just enable it
- there is a link provided, which gives the details of the acceleration provided

creating statis web site
- create a bucket
- If you want to use URL to configure in Route53, you have to give the same domain name in Route 53 (DNS name) as well.
- once the bucket is created, go to properties -> static website hosting
- The URL will be http://<name given in the bucket>.s3-website-<region name>.amazonaws.com
- click 'use this bucket to host a website'
- give index document - front web page
- also give error page
- upload html files of the same names

EC2
====
- on demand : fixed rate by hour (or second) without any commitment
	- used for users without up-front payment and long term commitment
	- applications with short term, spiky or un predictable workloads which cannot be interrupted
	- first timers
	- 
- Reserved : capacity reservation with discount on hourly charges for an instance. 1 year or 3 years
	- applicatoins with steady state or predictable use
	- that requires reserved capacity
	- up front payment to make profit
	- we can change the plans from CPU intense to memory intense
	- 
- Spot : enabled you to bid the price for the instance you want to purchace
	- onyl for a specific time
	- very cheap
	- 
- dedicated hosts : dedicated one. reduces the cost by allowing you to use existing server-bound s/w licenses
	- does not support multi tenancy
	- can be purchased hourly
Letters shows the family, numbers the version
- F1 : Field programming gateway : Genomic research, financial analytics, real time video processing, big data
- I3 : High speed storage : NoSQL DBs, data warehousing
- G3 : Graphics intensive : Video encoding/ 3D application streaming
- H1 : High Disk throughput : Map reduce based work loads, distributed file systems such as HDFS and MapR-FS
- T2 : Lowest cost, general purpose : Web server, Small DBs
- D2 : Dense storage : File servers, Data warehousing, Hadoop
- R4 : Memory optimized : Memory intensive Apps/DBs
- M5 : General purpose : App servers
- C5 : Compute optimized : CPU intensive Apps/DBs
- P3 : Graphics/General purpose GPU : Machine learning, Bit coin mining
- X1 : Memory optimized : SAP HANA/Apache Spark

DR. MC GIFT PX
D - Density
R - RAM
M - Main choice for gerenal purpose apps
C - Compute
G - Graphics
I - IOPS
F - FPGA
T - cheap general purpose (Think T2 micro)
P - Graphics (Think pics)
X - Xtreme memory

FIGHT DR MC PX

EBS stands for elastic block storage
drive where OS installed is called root
EBS volume types
- general purpose SSD (GP2)
	- ratio of 3 IOPS per GB with upto 10,000 IOPS and abilit to burst up to 3000 IOPS for extended periods of time for volumes at 3334 GiB and above
- Provisioned IOPS SSD (IO1)
	- where you needs more than 10000 IOPS - 20000 IOPS
- Throughput optimized HDD (ST1)
	- big data, data warehouse, log processing
- Cold HDD 
	- lowest cost storage for infrequent access
- Magenetic
	- cheapest
- 

- VPC stands for virtual private cloud
- one sub-net is mapped to one availability zone
- there is option to stop accidental termination
- detailed monitoring means monitoring per minute, usual is every five minutes
- boot strap scripts can be passed through advanced options
- root volume could be general purpose SSD, provisioned IOPS SSD or magnetic
- EBS volume can be general purpose SSD, provisioned IOPS SSD, Cold HDD, Throughput optimised HDD or magnetic
- usually root volumes cannot be encrypted. for that, we need to use a third party tool. or can be done using AMI's in the AWS console or uding API
- if you are using through windows, you need a putty
- user for connecting is ec2-user
- downloaded private key file is .pem, however to use with Putty, you got to convert it to .ppk format
- user puttygen application for that
- yum update is required for getting updates; always do that
- to create a static page, place the page in server using TCP
- yum install httpd -y
- service httpd start
- any change we do to security group reflects immediately
- security groups are made for restricting access to servers
- all the rules, we set for security group will be applicable for all its members
- all inboud rules have automtic out bound rules; you need not set that explicitly. security groups are stateful unlike ACL
- "all servers" are mapped as '0.0.0.0/0"
- all protocals are blocked by dafault, you can only make them available 
- if you want to block IP, use ACL
- we cannot have EC2 instance in one availability zone and its respective ESB volume in another availability zone
- standard volume types cannot be modified; they are just magnetic and the oldest
- if you want to copy one volume to another availability zone, create a snap shot > create a volume > select another availability zone
- Also, we can move the snapshot to another region
- Also you can create an image (AMI) out of snapshot; same image can be made out of the EC2 instance directly
- this AMI can be moved across to another region
- deleting EC2 instance only deletes root volume. for deleting other volumes, you have to explicitly do it
- you cannot share encrypted volumes or make it public

- RAID : Redundant Array of Independent Disks
- this is a set of disks actting as a single disk
- RAID 0 : this is a series of disks with high performance. However if we lose of data of one disk, we will lose entire data
- RAID 1 : is with redundancy. One disk is mirrored into another disk. this is safer
- RAID 5 : is when you have multiple disk and you create a checksum. if one disk fails, data can be recreated from this checksum. Amazon discourages usage of this
- RAID 10 : is a combination of RAID 1 and 0. This is striped and mirrored. has good performance and good redundancy
- In AWS, you create a series of volume and RAID it
- for doing anything with RAID, edit the security group and add RDP
- when you are using wondows, password you have to get explictly. there is option available along with instance list
- you create windows disks, login to windows and delete partitions. after that right click and mention as RAID 5 and done
- when you take a snapshot of a RAID, you may lose the cached data. This can be solved by putting application consistent snapshot. for that:
	- you have to stop all app writings
	- flush the cache
this can be achieved using
	- freeze the file system
	- unmount the RAID array
	- shutting down the associated EC2 instances
	- take the snapshot
= You cam take an AMI of root folder of the EC2 instance created, encrypt ( encrypt while copying the AMI) and copy to another region. Start that instance and you get a EC2 instance running in amother region
- also we an purchase the AMIs from market place
- you select AMI based on 
	- Region
	- OS
	- Architecture
	- Launch Permissions
	- Storage for the root device
		- Instance store (EPHEMERAL storage) : root volume is from an AMI instance created from a template stored in Amazon S3
		- EBS backed volumes :root device is launched from the AMI is an Amazon EBS volume created from an Amazon snapshot
- instance based stores are based on VM. so if the host fails, the instance also fails. it is less durable
- instance volumes cannot be stopped
- insance volume stopping can lose the data, but ESB does not
- while stopping the instance, EBS can ask to kee the root volume

39)
- Load balancers
	- Application load balancers
	- Network load balancers
	- Classic load balancers
- application load balancers are intelligent. here one set of service will be given for a spcific job.load balancer will understand the request type and will route the request to the righ set. Tesla model X set of services manages, Model X service reqquests and Mdel M services manage M requests. They work on layer 7
- network load balancers are maintained for extreme performances. will be able to handle millions of requests with minimu network latency. They work on layer 4. Tesla roadster
- classic load balancers are a mix of both. this uses the properties of layer 4 like sticky sessions and x-forwarded. also uses layer 4 balancing for applications that rely prely on the TCP protocol. These are usually called Elastic load balancers (ELBs)
- if you get a 504 error in classic load balancer, it means that your server is taking too much time to respond. This could be due to application of DB
- when your app server is behind your load balancer, your application server only gets the private IP (IP of the load balancer). This is not the right thing, since the application server gets a lot of information from the original IP. The original IP, we get as part of 'X-forwarded-for' header.
- health status check - can be done by creating a load balancer and assigning health check routine
- response time out : time till which it should wait for response
- interval : how often you should ping
- Unhealthy threshold : how many concecutive times you have to check before declare service as unhealthy
- healthy threshold : How many concecutive times you have to check before you declare service as healthy
- for a load balancer, you always get a DNS name - not IP


Cloud watch
- standard monitoring is 5 minutes, detailed is one minute
- management tools > cloud watch
- under cloud watch, you can create dashboards
- add widgets to dashboard
- by default following metrices are available
	- CPU Credit Balance, CPU Credit Usage, CPU Utiliztion
	- Disk Read Bytes, Disk Write Bytes,Disk Read Ops, Disk Write Ops
	- Network In, Network Out, Network packets in, Network packets out
	- Status check Failed, Status check Failed Instance, Status check Failed System
- You can configure mail alert system for resources
- Also you can configure state change alerts for resources
- whenever a resource state changes, you can configure to add an event to even stream and route them to take a specific action
- Also, you can use rules to take action on a pre-determined schedule 
- cloud watch is for monitoring and cloud trail is for auditing  
 

42)AWS CLI
- create an EC2 instance
- create IAM > user with programatic access
- attach administrator policies to the user
- get access key id and secret access key
- do ssh to AWS (ssh ec2-user@<ip address>
- aws configure
- give access key id
- give secret access key
- give correct region
- give output format as blank
- aws s3 ls -> lists all buckets
- aws s3 help -> lists all aws commands
- mb command is for making a bucket
- cd ~ > cd .aws - you can see aws credentials saved in a file
- aws ec2 describe-instances - will list all instances running in EC2
- aws ec2 terminate-instances --instance-ids <instance id> - will stop the aws instance
- 

43) Identity Management roles
- go to IAM > roles
- you can allow users from other platforms like FB, to login to AWS using 'Role for identity procvider access'
- select the role type
- select policy
- role name and review
- all roles are global
- we can attach a role to a running instance
- you can either create a user or role for an EC2 instance. if you are using user, your credentials will be locally stored inside ~/.aws folder, which is a security threat. instead you can use role, which does not save any credentials locally. when you create user, you keep IAM role as none

44)S3 CLI
- if you want to copy any data outside your region, use region flag
- aws s3 cp --recursive s3://acloudguru-euwest2 /home/ec3-user --region eu-west-2

45) Bash scripting lasb
- while configuring EC2, under 'Configue Instance' tab, put start up script under 'Advanced Details' > user data. this will be executed every time the instance starts
- aws s3 cp s3://<bucket name>/index.html /var/www/html

46) EC2 instance metadata
- metadata of EC2 instance will be available under -
curl http://169.254.169.254/latest/meta-data/
curl httlp://169.254.169.254/latest/user-data :- will get you your boot strap script

47)Launch configurations and Auto scaling groups
- auto scaling is an option to given to server - more like fault tolerence system
- Auto scaling : EC2 dashboard > Auto Scaling > Launch Configuration
- you can just select the number of instances. Based on that multiple instances will be deployed
- while creating the auto scaling group, we can define the rule. we can say that if the CPU utlization is greater than 90 % one instance has to be launched. then we need to wait for a cooling period and repeat the process. this will be repeated till we have launched all instances

48)Placement groups
- placement groups are of two kinds :
	- clustered placement group
	- spread placement group
- clustered placement group is default and it is always in a single availability zone
- spread placement groups machines are placed under different set of hardare independent of each other
- all members of a placment group should be homogenous
- cant merge placement groups
- cant move an existing instance to a placement grou[
- only certain set can be placed in placement groups - compute optimized, GPU, memory optimized, storage optimized

 